from fastapi import FastAPI, Query
import pandas as pd
import os
import re
import nltk
import sqlite3
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

app = FastAPI()

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()


def clean_text(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in stop_words]
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    tokens = [stemmer.stem(t) for t in tokens]
    return " ".join(tokens)


# âœ… Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ data
DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data', 'ir_docs.db'))

@app.get("/load-docs-to-db")
def load_docs_to_db(
    dataset: str = Query("trec_tot", description="Ø§Ø³Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (trec_tot Ø£Ùˆ antique)"),
    file_name: str = Query(..., description="ğŸ“„ Ø§Ø³Ù… Ù…Ù„Ù CSV (Ù…Ø«Ù„Ø§Ù‹ trec_tot_docs.csv)")
):
    base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data', dataset))
    csv_path = os.path.join(base_path, file_name)

    if not os.path.exists(csv_path):
        return {"error": f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {csv_path}"}

    df = pd.read_csv(csv_path)
    if "doc_id" not in df.columns or "doc_text" not in df.columns:
        return {"error": "âŒ Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: doc_id, doc_text"}

    df["clean_text"] = ""  # Ø¹Ù…ÙˆØ¯ ÙØ§Ø±Øº Ù…Ø¨Ø¯Ø¦ÙŠØ§Ù‹
    table_name = f"{dataset}_documents"

    conn = sqlite3.connect(DB_PATH)
    df.to_sql(table_name, conn, if_exists="replace", index=False)
    conn.close()

    return {
        "âœ…": "ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
        "dataset": dataset,
        "table": table_name,
        "documents": len(df)
    }


@app.get("/clean-docs-in-db")
def clean_docs_in_db(
    dataset: str = Query("trec_tot", description="Ø§Ø³Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (trec_tot Ø£Ùˆ antique)")
):
    table_name = f"{dataset}_documents"

    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql(f"SELECT * FROM {table_name}", conn)

    if "doc_text" not in df.columns or "doc_id" not in df.columns:
        conn.close()
        return {"error": f"âŒ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: doc_id, doc_text"}

    df["clean_text"] = df["doc_text"].astype(str).apply(clean_text)

    df.to_sql(table_name, conn, if_exists="replace", index=False)
    conn.close()

    # Ø­ÙØ¸ Ù†Ø³Ø®Ø© CSV ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
    base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data', dataset))
    os.makedirs(base_path, exist_ok=True)
    clean_csv_path = os.path.join(base_path, f"{dataset}_docs_clean.csv")
    df.to_csv(clean_csv_path, index=False)

    return {
        "âœ…": "ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙˆØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ­ÙØ¸ Ù†Ø³Ø®Ø© CSV",
        "dataset": dataset,
        "table": table_name,
        "documents_cleaned": len(df),
        "clean_csv_path": clean_csv_path
    }


@app.get("/save-docs-to-db")
def save_docs_to_db(
    dataset: str = Query("trec_tot", description="Ø§Ø³Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: trec_tot Ø£Ùˆ antique"),
    file_name: str = Query(None, description="ğŸ“„ Ø§Ø³Ù… Ù…Ù„Ù CSV Ø§Ù„Ù…Ù†Ø¸Ù (Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø­Ø³Ø¨ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)")
):
    # ØªØ¹ÙŠÙŠÙ† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
    if dataset == "trec_tot":
        file_name = file_name or "trec_tot_docs_clean.csv"
        table_name = "trec_tot_documents"
    elif dataset == "antique":
        file_name = file_name or "antique_docs_clean.csv"
        table_name = "antique_documents"
    else:
        return {"error": f"âŒ Ø§Ø³Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {dataset}"}

    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù…Ù„Ù
    base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data', dataset))
    file_path = os.path.join(base_path, file_name)

    if not os.path.exists(file_path):
        return {"error": f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}"}

    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df = pd.read_csv(file_path)

    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    if 'doc_id' not in df.columns or 'doc_text' not in df.columns or 'clean_text' not in df.columns:
        return {"error": "âŒ Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: doc_id, doc_text, clean_text"}

    # Ø­ÙØ¸ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    try:
        conn = sqlite3.connect(DB_PATH)
        df.to_sql(table_name, conn, if_exists='replace', index=False)
        conn.close()
    except Exception as e:
        return {"error": f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸: {str(e)}"}

    return {
        "âœ…": "ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­",
        "dataset": dataset,
        "table_name": table_name,
        "stored_documents": len(df),
        "db_path": DB_PATH
    }


@app.get("/show-docs-from-db")
def show_docs_from_db(
    dataset: str = Query("trec_tot", description="Ø§Ø³Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (trec_tot Ø£Ùˆ antique)"),
    limit: int = Query(10, description="Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¹Ø±Ø¶Ù‡Ø§")
):
    table_name = f"{dataset}_documents"

    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql(f"SELECT * FROM {table_name} LIMIT {limit}", conn)
    conn.close()

    return {
        "dataset": dataset,
        "shown_documents": len(df),
        "documents": df.to_dict(orient="records")
    }

@app.get("/clean-queries")
def clean_queries(
    dataset: str = Query("trec_tot", description="Ø§Ø³Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (trec_tot Ø£Ùˆ antique)"),
    file_name: str = Query(None, description="ğŸ“„ Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø­Ø³Ø¨ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)")
):
    import sqlite3

    # ğŸ“ Ù…Ù„Ù Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙŠØªÙ… Ù‚Ø±Ø§Ø¡ØªÙ‡ Ù…Ù†:
    # Documents/ir_project2/data/{dataset}/{dataset}_queries.csv
    if file_name is None:
        file_name = f"{dataset}_queries.csv"

    # ğŸ—‚ï¸ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data', dataset))

    # ğŸ“„ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù…Ù„Ù Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠ
    file_path = os.path.join(base_path, file_name)

    # âŒ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù
    if not os.path.exists(file_path):
        return {"error": f"âŒ Ù…Ù„Ù Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}"}

    # ğŸ“„ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
    df = pd.read_csv(file_path)

    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    if "query_id" not in df.columns or "query_text" not in df.columns:
        return {"error": "âŒ Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: query_id, query_text"}

    # ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
    df["clean_text"] = df["query_text"].astype(str).apply(clean_text)

    # ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ù†Ø¸ÙØ© ÙÙŠ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯:
    # Documents/ir_project2/data/{dataset}/{dataset}_queries_clean.csv
    cleaned_file_path = os.path.join(base_path, f"{dataset}_queries_clean.csv")
    df.to_csv(cleaned_file_path, index=False)

    query_table = f"{dataset}_queries"

    try:
        conn = sqlite3.connect(DB_PATH)
        df.to_sql(query_table, conn, if_exists='replace', index=False)
        conn.close()
    except Exception as e:
        return {
            "âœ…": "ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙˆØ­ÙØ¸Ù‡Ø§ ÙÙŠ Ù…Ù„ÙØŒ Ù„ÙƒÙ† Ø­ØµÙ„ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
            "cleaned_file": cleaned_file_path,
            "error": str(e)
        }

    return {
        "âœ…": "ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ù†Ø¬Ø§Ø­ØŒ ÙˆØ­ÙØ¸Ù‡Ø§ ÙƒÙ…Ù„Ù ÙˆÙƒØ¬Ø¯ÙˆÙ„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
        "original_file": file_path,
        "cleaned_file": cleaned_file_path,
        "db_table": query_table,
        "queries_cleaned": len(df)
    }
