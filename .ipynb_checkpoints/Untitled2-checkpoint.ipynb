{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a180c56-dc02-46f1-a2ae-a3b74607e679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ 150 Ø§Ø³ØªØ¹Ù„Ø§Ù…\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m     qid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     45\u001b[0m     query_text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 46\u001b[0m     results \u001b[38;5;241m=\u001b[39m search_query(query_text)\n\u001b[0;32m     47\u001b[0m     run_before[qid] \u001b[38;5;241m=\u001b[39m {doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]: doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m results}\n\u001b[0;32m     48\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36msearch_query\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     33\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:8001/query-match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     35\u001b[0m     url,\n\u001b[0;32m     36\u001b[0m     params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k}\n\u001b[0;32m     37\u001b[0m )\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'results'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from ranx import Run, Qrels, evaluate\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ===== Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ =====\n",
    "dataset = \"trec_tot\"  # â† ØºÙŠÙ‘Ø±Ù‡Ø§ Ø¥Ù„Ù‰ \"antique\" Ù„Ø§Ø­Ù‚Ù‹Ø§ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©\n",
    "top_k = 10\n",
    "queries_file = f\"data/{dataset}/{dataset}_queries.csv\"\n",
    "qrels_file = f\"data/{dataset}/{dataset}_qrels.csv\"\n",
    "\n",
    "# ===== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª =====\n",
    "df_queries = pd.read_csv(queries_file)\n",
    "print(f\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(df_queries)} Ø§Ø³ØªØ¹Ù„Ø§Ù…\")\n",
    "\n",
    "# ===== ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø·Ù„Ø¨ =====\n",
    "def refine_query(query: str):\n",
    "    url = \"http://127.0.0.1:8000/refine-query\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"options\": {\n",
    "            \"spelling_correction\": True,\n",
    "            \"query_expansion\": True,\n",
    "            \"query_suggestion\": True\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(f\"{url}?dataset={dataset}\", json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def search_query(query: str):\n",
    "    url = \"http://127.0.0.1:8001/query-match\"\n",
    "    response = requests.get(\n",
    "        url,\n",
    "        params={\"query\": query, \"dataset\": dataset, \"top_k\": top_k}\n",
    "    )\n",
    "    return response.json()[\"results\"]\n",
    "\n",
    "# ===== Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø¨Ø¯ÙˆÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… =====\n",
    "run_before = {}\n",
    "start = time.time()\n",
    "for _, row in df_queries.iterrows():\n",
    "    qid = str(row[\"query_id\"])\n",
    "    query_text = row[\"query_text\"]\n",
    "    results = search_query(query_text)\n",
    "    run_before[qid] = {doc[\"doc_id\"]: doc[\"score\"] for doc in results}\n",
    "end = time.time()\n",
    "print(f\"âŒ› Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ù…Ø³ØªØºØ±Ù‚ Ø¨Ø¯ÙˆÙ† ØªØ­Ø³ÙŠÙ†: {end - start:.2f} Ø«Ø§Ù†ÙŠØ©\")\n",
    "\n",
    "# ===== Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø¨Ø¹Ø¯ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… =====\n",
    "run_after = {}\n",
    "start = time.time()\n",
    "for _, row in df_queries.iterrows():\n",
    "    qid = str(row[\"query_id\"])\n",
    "    query_text = row[\"query_text\"]\n",
    "    refined = refine_query(query_text)[\"refined_query\"]\n",
    "    results = search_query(refined)\n",
    "    run_after[qid] = {doc[\"doc_id\"]: doc[\"score\"] for doc in results}\n",
    "end = time.time()\n",
    "print(f\"âŒ› Ø§Ù„Ø²Ù…Ù† Ø§Ù„Ù…Ø³ØªØºØ±Ù‚ Ø¨Ø¹Ø¯ ØªØ­Ø³ÙŠÙ†: {end - start:.2f} Ø«Ø§Ù†ÙŠØ©\")\n",
    "\n",
    "# ===== Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ranx =====\n",
    "qrels = Qrels.from_file(qrels_file, kind=\"trec\")\n",
    "run_before_r = Run(run_before, name=\"Before Refinement\")\n",
    "run_after_r = Run(run_after, name=\"After Refinement\")\n",
    "\n",
    "results = evaluate(\n",
    "    qrels=qrels,\n",
    "    runs=[run_before_r, run_after_r],\n",
    "    metrics=[\"map@10\", \"ndcg@10\", \"recall@10\"]\n",
    ")\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d706f719-fa24-483d-a606-fac3ec584d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 150 queries\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'query'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df_queries\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     42\u001b[0m     qid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 43\u001b[0m     query \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m         results \u001b[38;5;241m=\u001b[39m search_query(query)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'query'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from ranx import Run, Qrels, evaluate\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ğŸŸ¦ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
    "dataset = \"trec_tot\"  # â† ØºÙŠÙ‘Ø± Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¥Ù„Ù‰ \"antique\"\n",
    "top_k = 10\n",
    "\n",
    "queries_file = f\"data/{dataset}/{dataset}_queries.csv\"\n",
    "qrels_file = f\"data/{dataset}/{dataset}_qrels.tsv\"\n",
    "\n",
    "# ğŸŸ¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "df_queries = pd.read_csv(queries_file)\n",
    "print(f\"âœ… Loaded {len(df_queries)} queries\")\n",
    "\n",
    "# ğŸŸ¦ Ø®Ø¯Ù…Ø§Øª Ø®Ø§Ø±Ø¬ÙŠØ©\n",
    "def refine_query(query_text):\n",
    "    url = \"http://127.0.0.1:8000/refine-query\"\n",
    "    payload = {\n",
    "        \"query\": query_text,\n",
    "        \"options\": {\n",
    "            \"spelling_correction\": True,\n",
    "            \"query_expansion\": True,\n",
    "            \"query_suggestion\": True\n",
    "        }\n",
    "    }\n",
    "    response = requests.post(f\"{url}?dataset={dataset}\", json=payload)\n",
    "    return response.json()[\"refined_query\"]\n",
    "\n",
    "def search_query(query_text):\n",
    "    url = \"http://127.0.0.1:8001/query-matching\"\n",
    "    response = requests.get(url, params={\"query\": query_text, \"dataset\": dataset, \"top_k\": top_k})\n",
    "    return response.json()[\"results\"]\n",
    "\n",
    "# ğŸŸ¨ ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ø³ÙŠÙ†\n",
    "run_before = {}\n",
    "start_time = time.time()\n",
    "for _, row in df_queries.iterrows():\n",
    "    qid = str(row[\"query_id\"])\n",
    "    query = row[\"query\"]\n",
    "    try:\n",
    "        results = search_query(query)\n",
    "        run_before[qid] = {res[\"doc_id\"]: res[\"score\"] for res in results}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… {qid}: {e}\")\n",
    "end_time = time.time()\n",
    "print(f\"â±ï¸ Ø²Ù…Ù† Ø§Ù„Ø¨Ø­Ø« Ø¨Ø¯ÙˆÙ† ØªØ­Ø³ÙŠÙ†: {end_time - start_time:.2f} Ø«ÙˆØ§Ù†ÙŠ\")\n",
    "\n",
    "# ğŸŸ© ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ†\n",
    "run_after = {}\n",
    "start_time = time.time()\n",
    "for _, row in df_queries.iterrows():\n",
    "    qid = str(row[\"query_id\"])\n",
    "    query = row[\"query\"]\n",
    "    try:\n",
    "        refined_query = refine_query(query)\n",
    "        results = search_query(refined_query)\n",
    "        run_after[qid] = {res[\"doc_id\"]: res[\"score\"] for res in results}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… {qid}: {e}\")\n",
    "end_time = time.time()\n",
    "print(f\"â±ï¸ Ø²Ù…Ù† Ø§Ù„Ø¨Ø­Ø« Ø¨Ø¹Ø¯ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {end_time - start_time:.2f} Ø«ÙˆØ§Ù†ÙŠ\")\n",
    "\n",
    "# ğŸŸ¦ Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ØµÙŠØºØ© JSON (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)\n",
    "output_dir = f\"results/{dataset}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{output_dir}/run_before_refinement.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_before, f, indent=2)\n",
    "\n",
    "with open(f\"{output_dir}/run_after_refinement.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_after, f, indent=2)\n",
    "\n",
    "print(\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ø¬Ù„Ø¯:\", output_dir)\n",
    "\n",
    "# ğŸŸ¦ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ranx\n",
    "qrels = Qrels.from_file(qrels_file, kind=\"trec\")\n",
    "run_b = Run(run_before, name=\"Before_Refinement\")\n",
    "run_a = Run(run_after, name=\"After_Refinement\")\n",
    "\n",
    "results = evaluate(\n",
    "    qrels=qrels,\n",
    "    runs=[run_b, run_a],\n",
    "    metrics=[\"map@10\", \"ndcg@10\", \"recall@10\"]\n",
    ")\n",
    "\n",
    "# ğŸŸ© Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ…Ù‚Ø§Ø±Ù†Ø©\n",
    "pd.DataFrame(results).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7797bf1b-093e-4f65-84f1-0f97d2d85bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: httpx in c:\\programdata\\anaconda3\\lib\\site-packages (0.27.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "pip install httpx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb857cf-15be-4db8-bcc5-37ea3e994ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     query_id                                         query_text\n",
      "0         152  Movie from  the early 2000s I believe about th...\n",
      "1         531  Alright so I saw this movie sometime in the ea...\n",
      "2         473  This is an older 80 s movie. Maybe early 90s. ...\n",
      "3         659  I donâ€™t really have any memory of this film. I...\n",
      "4        1095  I remember seeing a trailer for a fantasy movi...\n",
      "..        ...                                                ...\n",
      "145      1077  So, the movie is about this weird virus or som...\n",
      "146       834  I remember a  movie this is what I can remembe...\n",
      "147       325  There was a movie I saw on tv sometime in the ...\n",
      "148       385  Iâ€™m trying to find the name of a film, whereby...\n",
      "149       521  Thereâ€™s an old one I got to see just once as a...\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ğŸŸ¦ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
    "dataset = \"trec_tot\"  # â† ØºÙŠÙ‘Ø± Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¥Ù„Ù‰ \"antique\"\n",
    "top_k = 10\n",
    "\n",
    "queries_file = f\"data/{dataset}/{dataset}_queries.csv\"\n",
    "\n",
    "\n",
    "# ğŸŸ¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª\n",
    "df_queries = pd.read_csv(queries_file)\n",
    "print(df_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134db0c8-83fb-4241-b966-0a9addb84485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
