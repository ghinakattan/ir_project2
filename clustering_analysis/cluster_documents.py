import os
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score
)
from umap import UMAP
from scipy.sparse import load_npz

# === Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ===
dataset = "trec_tot"  # Ø£Ùˆ "antique"
project_root = os.getcwd()

# Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ Ø£Ø±Ø³Ù„ØªÙ‡Ø§ØŒ Ù…Ù„ÙØ§Øª tfidf Ø¶Ù…Ù† offline_indexing_service
base_offline_indexing_path = os.path.join(project_root, "offline_indexing_service", "data", dataset)

vectorizer_path = os.path.join(base_offline_indexing_path, "tfidf_vectorizer.pkl")
tfidf_matrix_path = os.path.join(base_offline_indexing_path, "tfidf_docs_matrix.npz")

# Ù…Ù„ÙØ§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ù…Ù†Ø¸ÙØ© Ø¶Ù…Ù† Ù…Ø¬Ù„Ø¯ data Ù…Ø¨Ø§Ø´Ø±Ø© (ÙˆÙ„ÙŠØ³ Ø¶Ù…Ù† offline_indexing_service)
base_data_path = os.path.join(project_root, "data", dataset)

if dataset == "trec_tot":
    doc_file = os.path.join(base_data_path, "trec_tot_docs_clean.csv")
elif dataset == "antique":
    doc_file = os.path.join(base_data_path, "antique_docs_clean.csv")
else:
    doc_file = None  # Ø£Ùˆ Ø±ÙØ¹ Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ù‡Ù†Ø§

print("vectorizer_path:", vectorizer_path)
print("tfidf_matrix_path:", tfidf_matrix_path)
print("doc_file:", doc_file)

# === ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===
print("ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…ØµÙÙˆÙØ©...")
vectorizer = joblib.load(vectorizer_path)
tfidf_matrix = load_npz(tfidf_matrix_path)

print(f"âœ… Ø´ÙƒÙ„ Ø§Ù„Ù…ØµÙÙˆÙØ©: {tfidf_matrix.shape}")
doc_ids = pd.read_csv(doc_file)["doc_id"].astype(str).tolist()

 # === ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… UMAP (Ù„Ù„Ø¹Ø±Ø¶ ÙÙ‚Ø·) ===
print("ğŸ“‰ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… UMAP...")
sample_size = min(5000, tfidf_matrix.shape[0])
sample_indices = np.random.choice(tfidf_matrix.shape[0], sample_size, replace=False)
sample_matrix = tfidf_matrix[sample_indices]

umap = UMAP(n_components=2, n_neighbors=30, min_dist=0.0, metric='cosine', random_state=42)
reduced_data = umap.fit_transform(sample_matrix)

# === Ø¹Ù†Ù‚Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KMeans Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ÙˆÙ„ÙŠØ³ ÙÙ‚Ø· Ø§Ù„Ø¹ÙŠÙ†Ø©) ===
print("ğŸ”„ ØªÙ†ÙÙŠØ° MiniBatchKMeans Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚...")
n_clusters = 5
kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=512, random_state=42)
full_labels = kmeans.fit_predict(tfidf_matrix)

# === Ø¹Ø±Ø¶ Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ Ø¹Ù†Ù‚ÙˆØ¯ (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª) ===
print("\nğŸ“Œ Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ Ø¹Ù†Ù‚ÙˆØ¯:")
terms = vectorizer.get_feature_names_out()
for i in range(n_clusters):
    cluster_indices = np.where(full_labels == i)[0]
    cluster_matrix = tfidf_matrix[cluster_indices].mean(axis=0).A1
    top_indices = cluster_matrix.argsort()[::-1][:10]
    top_terms = [terms[j] for j in top_indices]
    print(f"\nCluster {i}:")
    print(", ".join(top_terms))

# === Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± Ù„ÙƒÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ===
print("\nğŸ’¾ Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹...")
docs_df = pd.DataFrame({
    "doc_id": doc_ids,  # Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
    "cluster_label": full_labels
})
results_path = os.path.join(base_offline_indexing_path, "tfidf_clusters.csv")
docs_df.to_csv(results_path, index=False)
print(f"ğŸ“ ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙÙŠ: {results_path}")

# === Ø­ÙØ¸ Ø§Ù„Ù…Ø®Ø·Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹ÙŠÙ†Ø© ÙÙ‚Ø· (ÙƒÙ…Ø§ Ù‡Ùˆ) ===
print("ğŸ“Š Ø±Ø³Ù… Ø§Ù„Ø¹Ù†Ù‚ÙˆØ¯...")
plt.figure(figsize=(10, 7))
colors = plt.cm.get_cmap('tab10', n_clusters)
sample_labels = full_labels[sample_indices]  # Ø®Ø° Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ù…Ù† ÙƒØ§Ù…Ù„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚

for i in range(n_clusters):
    cluster_points = reduced_data[sample_labels == i]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1],
                label=f"Cluster {i}", s=30, alpha=0.6, color=colors(i))

plt.title("ğŸ”µ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù†Ù‚ÙˆØ¯")
plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.legend()
plt.grid(True)
plt.tight_layout()

plot_path = os.path.join(base_offline_indexing_path, "clusters_plot.png")
plt.savefig(plot_path)
plt.show()

# === Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙƒÙ€ CSV ===
print("\nğŸ’¾ Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹...")
docs_df = pd.DataFrame({
    "doc_id": doc_ids,                 # Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
    "cluster_label": full_labels      # Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©
})
results_path = os.path.join(base_offline_indexing_path, "tfidf_clusters.csv")
docs_df.to_csv(results_path, index=False)
print(f"ğŸ“ ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙÙŠ: {results_path}")

# === Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ UMAP ===
umap_model_path = os.path.join(base_offline_indexing_path, "umap_model.joblib")
joblib.dump(umap, umap_model_path)
print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ UMAP ÙÙŠ: {umap_model_path}")

# === Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± KMeans ===
cluster_model_path = os.path.join(base_offline_indexing_path, "cluster_model.joblib")
joblib.dump(kmeans, cluster_model_path)
print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ KMeans ÙÙŠ: {cluster_model_path}")

print(f"\nâœ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§ÙƒØªÙ…Ù„. ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø®Ø·Ø· ÙÙŠ '{plot_path}'")
