import os
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score
)
from umap import UMAP
from scipy.sparse import load_npz

# === Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ===
dataset = "trec_tot"  # Ø£Ùˆ antique
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
base_path = os.path.join(project_root, 'data', dataset)
vectorizer_path = os.path.join(base_path, "tfidf_vectorizer.pkl")
tfidf_matrix_path = os.path.join(base_path, "tfidf_docs_matrix.npz")
doc_file = os.path.join(base_path, "tot_docs_clean.csv")  # âœ… ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ Ø­Ø³Ø¨ Ø·Ù„Ø¨Ùƒ

# === ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===
print("ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…ØµÙÙˆÙØ©...")
vectorizer = joblib.load(vectorizer_path)
tfidf_matrix = load_npz(tfidf_matrix_path)

print(f"âœ… Ø´ÙƒÙ„ Ø§Ù„Ù…ØµÙÙˆÙØ©: {tfidf_matrix.shape}")
doc_ids = pd.read_csv(doc_file)["doc_id"].astype(str).tolist()

# === ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… UMAP ===
print("ğŸ“‰ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… UMAP...")
umap = UMAP(n_components=2, n_neighbors=30, min_dist=0.0, metric='cosine', random_state=42)
sample_size = min(5000, tfidf_matrix.shape[0])
sample_indices = np.random.choice(tfidf_matrix.shape[0], sample_size, replace=False)
sample_matrix = tfidf_matrix[sample_indices]
reduced_data = umap.fit_transform(sample_matrix)

# === Ø¹Ù†Ù‚Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KMeans ===
print("ğŸ”„ ØªÙ†ÙÙŠØ° MiniBatchKMeans...")
n_clusters = 5
kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=512, random_state=42)
labels = kmeans.fit_predict(reduced_data)


# # âœ… Ø£Ø¶Ù Ø¨Ø¹Ø¯Ù‡ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± ğŸ‘‡
# joblib.dump(umap, os.path.join(base_path, "umap_model.joblib"))
# joblib.dump(kmeans, os.path.join(base_path, "clusters_model.joblib"))


# === Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„ÙƒÙ„ Ø¹Ù†Ù‚ÙˆØ¯ ===
print("\nğŸ“Œ Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ Ø¹Ù†Ù‚ÙˆØ¯:")
terms = vectorizer.get_feature_names_out()
for i in range(n_clusters):
    print(f"\nCluster {i}:")
    cluster_indices = np.where(labels == i)[0]
    cluster_matrix = sample_matrix[cluster_indices].mean(axis=0).A1
    top_indices = cluster_matrix.argsort()[::-1][:10]
    top_terms = [terms[j] for j in top_indices]
    print(", ".join(top_terms))

# === ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ù†Ù‚Ø¯Ø© ===
print("\nğŸ“ˆ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ù†Ù‚Ø¯Ø©:")
print(f"Silhouette Score: {silhouette_score(reduced_data, labels):.3f}")
print(f"Davies-Bouldin Index: {davies_bouldin_score(reduced_data, labels):.3f}")
print(f"Calinski-Harabasz Index: {calinski_harabasz_score(reduced_data, labels):.3f}")

# === Ø±Ø³Ù… Ø§Ù„Ø¹Ù†Ù‚ÙˆØ¯ ===
print("ğŸ“Š Ø±Ø³Ù… Ø§Ù„Ø¹Ù†Ù‚ÙˆØ¯...")
plt.figure(figsize=(10, 7))
colors = plt.cm.get_cmap('tab10', n_clusters)
for i in range(n_clusters):
    cluster_points = reduced_data[labels == i]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1],
                label=f"Cluster {i}", s=30, alpha=0.6, color=colors(i))

plt.title("ğŸ”µ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù†Ù‚ÙˆØ¯")
plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.legend()
plt.grid(True)
plt.tight_layout()

plot_path = os.path.join(base_path, "clusters_plot.png")
plt.savefig(plot_path)
plt.show()

print(f"âœ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§ÙƒØªÙ…Ù„. ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø®Ø·Ø· ÙÙŠ '{plot_path}'")
# import os
# import joblib
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# from sklearn.cluster import MiniBatchKMeans
# from sklearn.metrics import (
#     silhouette_score,
#     davies_bouldin_score,
#     calinski_harabasz_score
# )
# from umap import UMAP
# from scipy.sparse import load_npz

# # === Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ===
# dataset = "antique"  # ØªÙ… ØªØºÙŠÙŠØ±Ù‡ Ù„Ù€ antique
# base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data', dataset))
# vectorizer_path = os.path.join(base_path, "tfidf_vectorizer.pkl")
# tfidf_matrix_path = os.path.join(base_path, "tfidf_docs_matrix.npz")
# doc_file = os.path.join(base_path, f"{dataset}_docs_clean.csv")

# # === ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===
# print("ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…ØµÙÙˆÙØ©...")
# vectorizer = joblib.load(vectorizer_path)
# tfidf_matrix = load_npz(tfidf_matrix_path)

# print(f"âœ… Ø´ÙƒÙ„ Ø§Ù„Ù…ØµÙÙˆÙØ©: {tfidf_matrix.shape}")
# doc_ids = pd.read_csv(doc_file)["doc_id"].astype(str).tolist()

# # === ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… UMAP ===
# print("ğŸ“‰ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… UMAP...")
# umap = UMAP(n_components=2, n_neighbors=30, min_dist=0.0, metric='cosine', random_state=42)
# sample_size = min(5000, tfidf_matrix.shape[0])
# sample_indices = np.random.choice(tfidf_matrix.shape[0], sample_size, replace=False)
# sample_matrix = tfidf_matrix[sample_indices]
# reduced_data = umap.fit_transform(sample_matrix)

# # === Ø¹Ù†Ù‚Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KMeans ===
# print("ğŸ”„ ØªÙ†ÙÙŠØ° MiniBatchKMeans...")
# n_clusters = 5
# kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=512, random_state=42)
# labels = kmeans.fit_predict(reduced_data)

# # === Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù„ÙƒÙ„ Ø¹Ù†Ù‚ÙˆØ¯ ===
# print("\nğŸ“Œ Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ Ø¹Ù†Ù‚ÙˆØ¯:")
# terms = vectorizer.get_feature_names_out()
# for i in range(n_clusters):
#     print(f"\nCluster {i}:")
#     cluster_indices = np.where(labels == i)[0]
#     cluster_matrix = sample_matrix[cluster_indices].mean(axis=0).A1
#     top_indices = cluster_matrix.argsort()[::-1][:10]
#     top_terms = [terms[j] for j in top_indices]
#     print(", ".join(top_terms))

# # === ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ù†Ù‚Ø¯Ø© ===
# print("\nğŸ“ˆ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ù†Ù‚Ø¯Ø©:")
# print(f"Silhouette Score: {silhouette_score(reduced_data, labels):.3f}")
# print(f"Davies-Bouldin Index: {davies_bouldin_score(reduced_data, labels):.3f}")
# print(f"Calinski-Harabasz Index: {calinski_harabasz_score(reduced_data, labels):.3f}")

# # === Ø±Ø³Ù… Ø§Ù„Ø¹Ù†Ù‚ÙˆØ¯ ===
# print("ğŸ“Š Ø±Ø³Ù… Ø§Ù„Ø¹Ù†Ù‚ÙˆØ¯...")
# plt.figure(figsize=(10, 7))
# colors = plt.cm.get_cmap('tab10', n_clusters)
# for i in range(n_clusters):
#     cluster_points = reduced_data[labels == i]
#     plt.scatter(cluster_points[:, 0], cluster_points[:, 1],
#                 label=f"Cluster {i}", s=30, alpha=0.6, color=colors(i))

# plt.title("ğŸ”µ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù†Ù‚ÙˆØ¯ - Antique")
# plt.xlabel("UMAP-1")
# plt.ylabel("UMAP-2")
# plt.legend()
# plt.grid(True)
# plt.tight_layout()
# plt.savefig(os.path.join(base_path, "clusters_plot_antique.png"))
# plt.show()

# print("âœ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§ÙƒØªÙ…Ù„. ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø®Ø·Ø· ÙÙŠ 'clusters_plot_antique.png'")
